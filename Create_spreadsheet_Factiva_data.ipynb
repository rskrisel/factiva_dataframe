{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rskrisel/factiva_dataframe/blob/main/Create_spreadsheet_Factiva_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBUqcLruA8eI"
      },
      "outputs": [],
      "source": [
        "#https://jcldinco.medium.com/obtaining-and-cleaning-news-data-from-factiva-21a7a0ae2759\n",
        "#from factiva, select display/Full Article/Report plus Indexing\n",
        "#restrict data select based on region/publications\n",
        "#duplicates:identical\n",
        "#select articles, then click print. Then hit command+u for html code. Copy/paste code into VSCODE.\n",
        "#Save docs as htm\n",
        "\n",
        "import glob\n",
        "import pandas as pd\n",
        "# files = glob.glob('html/*.htm', recursive = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collecting news data from Factiva and saving it in a Dataframe"
      ],
      "metadata": {
        "id": "twshZ2KyZoZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "r9ye5YkcFJsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get data from Factiva, you must have a subscription to the Factiva database. Most universities subscribe to the database, so you can use your library to connect to it.\n",
        "\n",
        "Please consult with your librarian for how to properly search for articles in Factiva.\n",
        "\n",
        "Once you have narrowed down your search and found the articles you wish to work with, follow these steps:\n",
        "\n",
        "\n",
        "1.   Click \"display\" and select: Full Article/Report plus Indexing\n",
        "2.   Set the duplicates to identical.\n",
        "3. Select the articles you wish to collect, then click print/article format\n",
        "4. From the print view window, click `command` + `u` to view the html code.\n",
        "5. Copy the html code and store it in the `html_code` variable below.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MPEZAoZqbnuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "html_code= \"\"\"\n",
        "PASTE HTML CODE HERE\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "56l2ZC4Xb0bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you want to save the contents of the `html_code` variable into a `.htm` file:"
      ],
      "metadata": {
        "id": "gwezETiQOE1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Write the HTML code to the file\n",
        "with open(\"/content/drive/MyDrive/Factiva/factiva.htm\", 'w') as file: #replace with your path\n",
        "    file.write(html_code)\n",
        "\n",
        "\n",
        "# # For a list of variables with HTML content\n",
        "# html_list = [html_code1, html_code2, html_code3]  # Replace with your actual variables\n",
        "\n",
        "# # Iterate over the list and write each HTML content to a separate file\n",
        "# for i, html_code in enumerate(html_list):\n",
        "#     file_path = f\"/content/drive/MyDrive/Factiva/factiva_{i}.htm\"  # Replace with your path\n",
        "#     with open(file_path, 'w') as file:\n",
        "#         file.write(html_code)\n",
        "\n"
      ],
      "metadata": {
        "id": "M04OTM-rb555"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following line, you will look for all `.htm` files in your Factiva folder (and its subfolders, if any) located at the specified path on your Google Drive and return them as a list.\n",
        "\n",
        "- glob.glob is a function that finds all the files that match a specific pattern. In this case, it looks for files inside a folder called Factiva that have the .htm extension.\n",
        "- The part `/content/drive/MyDrive/Factiva/*.htm` is the path where it will look for the files. You would replace this with the path where your own files are located. The *.htm means it will find all files ending with .htm (which are likely HTML files).\n",
        "- recursive = True allows the function to search within subdirectories inside the Factiva folder as well.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EF3WgG0NOUFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = glob.glob('/content/drive/MyDrive/Factiva/*.htm', recursive = True) #replace with your path"
      ],
      "metadata": {
        "id": "1ieT2hcrFn3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoDNGSP7A8eL"
      },
      "outputs": [],
      "source": [
        "files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following `for loop` starts with an empty list. It then goes through each HTML file in the files list, reads any tables found in those files, and adds them to the empty_list.\n",
        "\n",
        "In this case, the goal is to have a list where each element is a dataframe, not a list of lists. Since `pd.read_html()` returns a list of dataframes for each file, `extend` is used to merge those dataframes directly into `empty_list` so that it contains all the dataframes in one flat structure.\n",
        "\n",
        "If you used `append`, you would end up with a nested structure where each element is a list of dataframes, which is likely not what you want."
      ],
      "metadata": {
        "id": "CfP8xPODQflX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tjD6vN0A8eL"
      },
      "outputs": [],
      "source": [
        "empty_list = []\n",
        "for file in files:\n",
        "    data = pd.read_html(file, index_col = 0) #reads the HTML content of the file and tries to find any tables inside it.\n",
        "    empty_list.extend(data) # The extend() method is used to add the data (which is a list of dataframes) from the current file to empty_list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBqYJq0QA8eM"
      },
      "outputs": [],
      "source": [
        "empty_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a variable, `frames`, which contains a dataframe where all relevant dataframes (those containing 'HD' in their index) are combined and flipped.\n",
        "\n",
        "In the next line of code, we will accomplish the following:\n",
        "- Look through all the dataframes in `empty_list` and selects only the ones where `HD` is found in the index.\n",
        "- Concatenate those dataframes side by side (combining their columns).\n",
        "- Finally, transpose the resulting dataframe, flipping the rows and columns, and assigning it to the variable `frames`.\n",
        "\n",
        "\n",
        "Breaking it down:\n",
        "- `[l for l in empty_list if 'HD' in l.index.values]:`\n",
        "1. This is a list comprehension. It goes through each item `l` in `empty_list` (which contains dataframes).\n",
        "2. For each dataframe `l`, it checks if `'HD'` is present in the index values of that dataframe `(l.index.values)`.\n",
        "3. If `'HD'` is found in the index of a dataframe, that dataframe is included in the resulting list. If `'HD'` is not found, that dataframe is ignored.\n",
        "\n",
        "- `pd.concat([...], axis=1)`:\n",
        "\n",
        "1. this concatenates (joins) all the dataframes that contain `'HD'` in their index. The `axis=1` argument means the dataframes will be concatenated side by side, meaning their columns will be combined.The result is a new dataframe where the data from each matching dataframe is merged by columns.\n",
        "\n",
        "- `.T`:\n",
        "1. This is a shorthand for \"transpose,\" which flips the rows and columns of the resulting dataframe.\n",
        "2. After concatenating the dataframes side by side, `.T` switches the rows and columns, so what were previously columns are now rows, and vice versa."
      ],
      "metadata": {
        "id": "rzjywplSRxIO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMiFyGxFA8eM"
      },
      "outputs": [],
      "source": [
        "frames = pd.concat([l for l in empty_list if 'HD' in l.index.values], axis=1).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7dWVPRlA8eM"
      },
      "outputs": [],
      "source": [
        "frames"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's drop unnecessary columns from the DataFrame and the rename certain columns to more meaningful or readable names (e.g., 'HD' becomes 'Headline', 'PD' becomes 'Publication_Date', etc.)."
      ],
      "metadata": {
        "id": "Bph-LCAFTw-h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nWDYnhuA8eN"
      },
      "outputs": [],
      "source": [
        "frames.drop(columns=['SC', 'CY','PUB', 'NS',\n",
        "                   'IPD', 'IPC',\n",
        "                   'IN', 'VOL', 'RF', 'LA', 'CO', 'AN', 'SE', 'WC', 'ED', 'PG', 'ART', 'CLM'  ], inplace = True)\n",
        "frames.rename(columns = {'HD': 'Headline',\n",
        "                         'PD': 'Publication_Date','SN': 'Source_Name', 'LP': 'Lead Paragraph',\n",
        "                          'TD': 'Body', 'ET':'Estimated_Time',\n",
        "                         'BY':'Author_Name', 'RE':'Region' }, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make sure our `Publication_Date` column is in datetime format."
      ],
      "metadata": {
        "id": "rewhDm1nT9as"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyleltLKA8eN"
      },
      "outputs": [],
      "source": [
        "frames['Publication_Date'] = pd.to_datetime(frames['Publication_Date'])\n",
        "frames.sort_values(by='Publication_Date', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next line of code is downloading a resource from the Natural Language Toolkit (nltk) package called 'punkt'. Here's a breakdown of what it does:\n",
        "\n",
        "Explanation:\n",
        "- `import nltk`:\n",
        "1. This imports the `nltk` library, which is a popular Python library for working with human language data (natural language processing).\n",
        "- `nltk.download('punkt')`:\n",
        "1. This downloads a tokenizer model called Punkt, which is used for splitting text into sentences or words.\n",
        "2. `Punkt` is a pre-trained model that comes with `nltk` and is used for tokenization tasks (breaking up text into smaller components, like sentences or words). Once downloaded, you'll be able to use it in conjunction with functions like `nltk.sent_tokenize()` to break up text into sentences or `nltk.word_tokenize()` to break it up into individual words.\n",
        "\n",
        "Why do you need this?\n",
        "You download this resource because `nltk` uses external models and corpora to process text. The `punkt` tokenizer is required for tasks like sentence splitting and word tokenization, which are fundamental for most natural language processing tasks."
      ],
      "metadata": {
        "id": "uNW38LVUUljI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcjkyvgIA8eO"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's import the `sent_tokenize` function from the `nltk.tokenize` module. This function is used to split a given text into individual sentences. It takes a string of text as input and returns a list of sentences."
      ],
      "metadata": {
        "id": "UC4KI022VFFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "OrLEP7E3MRAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we want to combine the text from the `Lead Paragraph` and the `Body` columns so we have the full article in a single cell."
      ],
      "metadata": {
        "id": "4lDj0U8MVOJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frames['CombinedText'] = frames['Lead Paragraph'] + \" \" + frames['Body']"
      ],
      "metadata": {
        "id": "6-VXXSniMSBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XHPcepOAWQSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's reset the index so it's the standard [0:] index, in ascending order."
      ],
      "metadata": {
        "id": "JaXGedRLWXxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = frames.reset_index()"
      ],
      "metadata": {
        "id": "ZHm89i49fO7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path= '/content/drive/MyDrive/Factiva'  # change to your path"
      ],
      "metadata": {
        "id": "RORSKp_qemwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will us a code that loops through each row in the DataFrame `df`, creates a unique text file for each row, and writes the content from the `CombinedText` column into that file. If the `CombinedText` value is missing (i.e., `NaN`), it writes an empty string instead. The filenames are generated dynamically based on the row index.\n",
        "\n",
        "In essence, it saves the text content from each row in the DataFrame as individual text files.\n",
        "\n",
        "1. **`for index, row in df.iterrows():`**\n",
        "   - This line starts a loop over each row in the DataFrame `df`.\n",
        "   - `df.iterrows()` is a pandas function that allows you to loop through the DataFrame row by row.\n",
        "   - `index` represents the row number (starting from 0), and `row` contains the data for that specific row in the form of a pandas Series.\n",
        "\n",
        "2. **`file_name = f\"{path}/text_file_{index + 1}.txt\"`**\n",
        "   - This line generates a unique filename for each row.\n",
        "   - The `f\"{path}/text_file_{index + 1}.txt\"` uses an f-string to create a file name based on the `index` (plus 1 to make it start at 1 instead of 0).\n",
        "   - `path` is a variable that contains the directory where the file will be saved (it should be defined earlier in the code).\n",
        "   - For example, for the first row (index 0), this would create a filename like `\"path/to/directory/text_file_1.txt\"`.\n",
        "\n",
        "3. **`with open(file_name, 'w') as file:`**\n",
        "   - This opens a file with the name `file_name` in write mode (`'w'`), allowing the program to write data into it.\n",
        "   - The `with` statement ensures the file is properly closed after writing, even if an error occurs.\n",
        "\n",
        "4. **`text_content = str(row['CombinedText']) if pd.notnull(row['CombinedText']) else ''`**\n",
        "   - This checks the value in the `'CombinedText'` column for the current row.\n",
        "   - **`pd.notnull(row['CombinedText'])`** checks if the value is **not** `NaN` (i.e., itâ€™s not missing).\n",
        "   - If the value is **not** `NaN`, it converts the value to a string using `str(row['CombinedText'])`.\n",
        "   - If the value is `NaN`, it sets `text_content` to an empty string (`''`).\n",
        "   - This ensures that no matter what value is in the `CombinedText` column, you will have valid text to write to the file (either the text itself or an empty string).\n",
        "\n",
        "5. **`file.write(text_content)`**\n",
        "   - This writes the `text_content` (the string version of the `CombinedText` value) to the file.\n",
        "   - If the `CombinedText` was `NaN`, it writes an empty string; otherwise, it writes the actual content from that column.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VOmnMvjjYcZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.iterrows():\n",
        "    file_name = f\"{path}/text_file_{index + 1}.txt\"  # Create a unique filename for each row\n",
        "    with open(file_name, 'w') as file:\n",
        "        text_content = str(row['CombinedText']) if pd.notnull(row['CombinedText']) else ''  # Convert to string and handle NaN\n",
        "        file.write(text_content)  # Write the text content to the file"
      ],
      "metadata": {
        "id": "jv2Cw3f9XJtj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}